"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[130],{7735(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2021/19/25/Dive - Into your docker images","metadata":{"permalink":"/blog/2021/19/25/Dive - Into your docker images","editUrl":"https://github.com/techslaves/techslaves.github.io/tree/master/website/blog/2021-19-25-Dive - Into your docker images.md","source":"@site/blog/2021-19-25-Dive - Into your docker images.md","title":"Dive - Into docker images","description":"Docker Image Layers","date":"2021-09-25T17:40:40.000Z","tags":[{"inline":true,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"Docker","permalink":"/blog/tags/docker"}],"readingTime":1,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Dive - Into docker images","excerpt":"In this post we will look into a tool called DIVE to understand and optimize docker images","date":"2021-09-25 17:40:40 +0530","tags":["Kubernetes","Docker"]},"unlisted":false,"nextItem":{"title":"Post 01 - Harbor High Availability deployment on Kubernetes","permalink":"/blog/2021/05/14/Harbor High Availability deployment on Kubernetes"}},"content":"**Docker Image Layers**\\n\\nUsually when a developer looks into docker images docker image history `<image-id>` is used to understand each layer, size of each layer and other image.\\n\\nReducing image size is a common problem which is faced while building docker image and to overcome this problem we follow best practices like using multi stage build , using smaller base image, use && with RUN command to reduce number of layers.\\n\\nDIVE is a tool which help in analysis of Image layers, Potential wasted Image, Image efficiency score. It help visualise each image layer and files added and modified in each layer.\\n\\nIt is supported on Windows, MacOs and Linux.\\n\\n![](/img/devops/dive/Dive_Image_Inspection.JPG)\\n\\nVarious option of DIVE usage:\\n\\n![](/img/devops/dive/Dive_Usage.JPG)\\n\\n\\nDIVE can also be integrated with CI tool to check if image passes highestUserWastedPercent (highest allowable percentage of bytes wasted), highestWastedBytes(highest allowable bytes wasted) and lowestEfficiency(lowest allowable image efficiency). If these criteria does not meet the expectation docker push to Docker Image repository will fail.\\n\\n![](/img/devops/dive/Dive_CI_Integration.JPG)\\n\\nIntegration will fail incase docker image does not meet the specified criteria.\\n\\n![](/img/devops/dive/Dive_CI_Failed.JPG)\\n\\nDIVE can also be optimsed by placing .dive.yaml file in home directory."},{"id":"/2021/05/14/Harbor High Availability deployment on Kubernetes","metadata":{"permalink":"/blog/2021/05/14/Harbor High Availability deployment on Kubernetes","editUrl":"https://github.com/techslaves/techslaves.github.io/tree/master/website/blog/2021-05-14-Harbor High Availability deployment on Kubernetes.md","source":"@site/blog/2021-05-14-Harbor High Availability deployment on Kubernetes.md","title":"Post 01 - Harbor High Availability deployment on Kubernetes","description":"In this post I will be covering details of deployment of Harbor High Availability on Bare Metal VM\'s. This will be a series of article covering","date":"2021-05-14T17:40:40.000Z","tags":[{"inline":true,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":3.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Post 01 - Harbor High Availability deployment on Kubernetes","excerpt":"In this post I will be covering details of deployment of Harbor High Availability on Bare Metal VM\'s.","date":"2021-05-14 17:40:40 +0530","image":"/img/kubernetes/harbor.png","tags":["Kubernetes"]},"unlisted":false,"prevItem":{"title":"Dive - Into docker images","permalink":"/blog/2021/19/25/Dive - Into your docker images"},"nextItem":{"title":"Post 04 - PostgreSQL High Availability deployment on Kubernetes","permalink":"/blog/2021/05/15/PostgeSQL High Availability deployment on Kubernetes"}},"content":"![](/img/kubernetes/harbor.png)\\n\\nIn this post I will be covering details of deployment of Harbor High Availability on Bare Metal VM\'s. This will be a series of article covering\\n\\n1) Harbor High Availability deployment on Kubernetes (this post)\\n\\n2) Kubernetes Dynamic Provisioning - Persistent Volume on demand (Using helm charts) \\\\\\n[Kubernetes Dynamic Provisioning - Persistent Volume on demand (Using helm charts)](2021-05-06-Dynamic%20Volume%20Provisioning%20NFS.md) \\\\\\\\\\n\\n3) High available Redis (Using helm charts) \\\\\\n\\n4) High available PostgreSQL database (Using helm charts) \\\\\\n[Post 04 - PostgreSQL High Availability deployment on Kubernetes](2021-05-15-PostgeSQL%20High%20Availability%20deployment%20on%20Kubernetes.md) \\\\\\n\\n5) Harbor High Availability deployment in action on Kubernetes using helm chart. \\\\\\n\\n> Harbor is an open source registry that secures artifacts with policies and role-based access control, \\nensures images are scanned and free from vulnerabilities, and signs images as trusted. \\nHarbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently\\n and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. - Source (Harbor.io)\\n\\n## Prerequisite\\n\\nTo install harbor using helm chart on Kubernetes cluster, we need to have \\n\\n#### *1) Kubernetes cluster 1.10+ :*\\nWe need to have Kubernetes cluster. Refer my previous post for this - [Bootstrap Kubernetes cluster with PV as NFS ](2021-01-14-Kubernetes.md)\\n\\n#### *2) Helm 2.8.0+ :* \\nI have installed helm in my local windows machine and using cluster config file from .kube folder to remotely connect to cluster.\\nWe can install helm chart on remote cluster using --kubeconfig parameter with value as path to config file.\\nEg: --kubeconfig=D:\\\\kubernetes\\\\config\\n\\n#### *3) High available PostgreSQL database :*\\nHarbor helm chart don\'t deploy PostgreSQL HA cluster and we need to pass IP address in values.yaml file of Harbor helm for its integration with Harbor.\\nThis can be achieved using bitnami helm chart from this link - [PostgreSQL helm](https://github.com/bitnami/charts/tree/master/bitnami/postgresql-ha)\\n\\n#### *4) High available Redis :*\\nHarbor helm chart don\'t deploy Redis HA cluster and we need to pass IP address in values.yaml file of Harbor helm for its integration with Harbor.\\nThis can be achieved using bitnami helm chart from this link - [Redis Helm](https://github.com/bitnami/charts/tree/master/bitnami/redis)\\n\\n\\n#### *5) PVC that can be shared across nodes or external object storage :*\\nI will be using NFS as shared storage device across nodes. Along with this I will configure NFS dynamic Provisioner for automatically creating PV on demand on the NFS.\\n\\n\\n## Now lets discuss on details of POD\'s created using Harbor helm chart:\\n\\nI did not enabled harbor-exporter for now. \\n\\n#### *1) harbor-chartmuseum -*\\nHarbor support storing helm chart along with docker Image. Incase you don\'t require helm chart support in your private Harbor repository you can disable this using harbor helm chart.\\n\\n#### *2) harbor-core -*\\nHarbor Core is one of the main components of Harbor. It interacts with redis cluster. So, incase redis cluster integration is not successful this pod will not come up and in turn harbor-jobservice pod will not come.\\n\\n#### *3) harbor-jobservice -* \\nHarbor jobservice is one of the main components of Harbor. This is the last pod to come up in all the pods of helm chart.\\n\\n#### *4) harbor-nginx -*\\nNginx container is itself a Reverse Proxy in front of the core and the portal containers. If Harbor is exposed as Ingress, then nginx pod is not deployed.\\n\\n#### *5) harbor-notary-server -*\\nNotary server is used for signing and verifying images. It is a optional pod and can be disabled used values.yaml in Harbor Helm charts.\\nThis is used when you want to sign you image while pushing. \\nDeveloper enable content trust and export server details.\\nexport DOCKER_CONTENT_TRUST=1 and export DOCKER_CONTENT_TRUST_SERVER=https://IP_ADDRESS:4443\\n\\n#### *6) harbor-notary-signer -*\\nNotary Signer cordinates with notary-server for signing image.\\n\\nIn the next post I will be configuring NFS-Dynamic provisioner which will create PV\'s on demand for PVC\'s created by Redis, PostgreSQL and Harbor Helm chart- \\\\\\n[Kubernetes Dynamic Provisioning - Persistent Volume on demand (Using helm charts)](2021-05-06-Dynamic%20Volume%20Provisioning%20NFS.md)"},{"id":"/2021/05/15/PostgeSQL High Availability deployment on Kubernetes","metadata":{"permalink":"/blog/2021/05/15/PostgeSQL High Availability deployment on Kubernetes","editUrl":"https://github.com/techslaves/techslaves.github.io/tree/master/website/blog/2021-05-15-PostgeSQL High Availability deployment on Kubernetes.md","source":"@site/blog/2021-05-15-PostgeSQL High Availability deployment on Kubernetes.md","title":"Post 04 - PostgreSQL High Availability deployment on Kubernetes","description":"In this post I will be covering details of deployment of PostgreSQL High Availability on Bare Metal VM\'s. This article is part of series of Harbor High Availability","date":"2021-05-14T17:40:40.000Z","tags":[{"inline":true,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":4.53,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Post 04 - PostgreSQL High Availability deployment on Kubernetes","excerpt":"In this post I will be covering details of deployment of PostgreSQL High Availability on Bare Metal VM\'s.","date":"2021-05-14 17:40:40 +0530","tags":["Kubernetes"]},"unlisted":false,"prevItem":{"title":"Post 01 - Harbor High Availability deployment on Kubernetes","permalink":"/blog/2021/05/14/Harbor High Availability deployment on Kubernetes"},"nextItem":{"title":"Post 02 - Kubernetes Dynamic Provisioning, Persistent Volume on demand (Using helm charts)","permalink":"/blog/2021/05/06/Dynamic Volume Provisioning NFS"}},"content":"In this post I will be covering details of deployment of PostgreSQL High Availability on Bare Metal VM\'s. This article is part of series of Harbor High Availability\\n\\n1) Harbor High Availability deployment on Kubernetes \\\\\\n[Post 01 - Harbor High Availability deployment on Kubernetes](2021-05-14-Harbor%20High%20Availability%20deployment%20on%20Kubernetes.md)\\\\\\n\\n2) Kubernetes Dynamic Provisioning : Persistent Volume on demand (Using helm charts) \\\\\\n[Kubernetes Dynamic Provisioning - Persistent Volume on demand (Using helm charts)](2021-05-06-Dynamic%20Volume%20Provisioning%20NFS.md) \\\\\\n\\n3) High available Redis (Using helm charts)\\n\\n4) High available PostgreSQL database (Using helm charts) (this post)\\n\\n5) Harbor High Availability deployment in action on Kubernetes using helm chart.\\n\\n> PostgreSQL is an open source object-relational database known for its reliability and data integrity. \\n\\n**Prerequisite**\\n\\n1) Kubernetes cluster is up and running. To know how to achieve this read my previous post bootstrap kubernetes using kubeadm.\\n\\n- [Bootstrap Kubernetes cluster with PV as NFS ](2021-01-14-Kubernetes.md)\\n\\n2) NameSpace with name harbor-private-registry is already existing.\\nIf its not created run:\\n\\n$ kubectl create ns harbor-private-registry\\n\\n3) Dynamic Volume Provisioning NFS is working and able to create pv on demand.\\n\\n- [Bootstrap Kubernetes cluster with PV as NFS ](2021-05-06-Dynamic%20Volume%20Provisioning%20NFS.md)\\n\\n***Using helm chart for PostgreSQL High Availability cluster***\\n\\nWe will be using bitnami helm chart for deploying High Availabilty PostgreSQL cluster - [PostgreSQL helm](https://github.com/bitnami/charts/tree/master/bitnami/postgresql-ha)\\n\\n#### We need to modify values.yaml file or override below values using --set flag while installing helm:\\n\\n*--set pgpool.replicaCount=2*\\n\\n*--set postgresql.replicaCount=3*\\n \\n*--set postgresql.existingSecret=postgresql-harbor-secret* : Name of secret created to store postgreSQL and repmgr password\\n\\n*--set pgpool.adminPassword=MTk5MCRwb3N0Z3Jlc3Fs*\\n\\n*--set pgpoolImage.tag=4.2.2-debian-10-r72*\\n\\n*--set postgresqlImage.tag=13.2.0-debian-10-r77*\\n\\n*--set global.storageClass=nfs-client* : Value (nfs-client) is name to storageClass created with NFS Dynamic Provisioner.\\n\\n*--set service.type=ClusterIP*\\n\\n#### Note : *Creating below Secret is important else you wont be able to reattach existing pv on next uninstall and reinstall of postgreSQL helm chart due to any reason.*\\n\\n#### Important Steps to create Secret (don\'t miss this)\\n\\nCreate secret for configurable parameter of bitnami postgreSQL HA Helm chart values.yaml **postgresql.existingSecret** \\n\\nCreate secret using below command and then install postgreSQL using helm chart:\\n\\n<script src=\\"https://gist.github.com/techslaves/a32ff5accc463cf68fba49f227588725.js\\"><\/script>\\n\\n\\nYou will get below output on successfull installation of helm chart:\\n\\n--------------------------------------------------------------------------------------------------------------------\\n\\n*NAME: postgresql-harbor-private-registry* \\\\\\n*LAST DEPLOYED: Fri May  15 12:19:43 2021* \\\\\\n*NAMESPACE: harbor-private-registry* \\\\\\n*STATUS: deployed* \\\\\\n*REVISION: 1* \\\\\\n*TEST SUITE: None* \\\\\\n*NOTES:* \\\\\\n\\n\\n*Please be patient while the chart is being deployed*\\n\\n*PostgreSQL can be accessed through Pgpool via port 5432 on the following DNS name from within your cluster:*\\n\\n*postgresql-harbor-private-registry-pgpool.harbor.svc.cluster.local*\\n\\n*Pgpool acts as a load balancer for PostgreSQL and forward read/write connections to the primary node while read-only connections are forwarded to standby nodes.*\\n\\n*To get the password for \\"postgres\\" run:*\\n\\n*export POSTGRES_PASSWORD=$(kubectl get secret --namespace harbor-private-registry postgresql-harbor-private-registry-secret -o jsonpath=\\"\\\\{.data.postgresql-password\\\\}\\"* | base64 --decode)\\n*To get the password for \\"repmgr\\" run:*\\n\\n*--command -- psql -h postgresql-harbor-private-registry-pgpool -p 5432 -U postgres -d postgres*\\n\\n*To connect to your database from outside the cluster execute the following commands:*\\n*psql -h 127.0.0.1 -p 5432 -U postgres -d postgres *\\n\\n---------------------------------------------------------------------------------------------------------------------\\n\\nOnce the pods are up and running exec into postgreSQL stateful set pod and run *psql -h 127.0.0.1 -p 5432 -U postgres -d postgres * \\n\\nEnter the password you get from *kubectl get secret --namespace harbor-private-registry postgresql-harbor-private-registry-secret -o jsonpath=\\"\\\\{.data.postgresql-password\\\\}\\" | base64 --decode *\\n\\nCreate following database for harbor. Tables will be created automatically when Harbor HA starts\\n\\nCREATE DATABASE notary_server;\\n\\nCREATE DATABASE notary_signer;\\n\\nCREATE DATABASE harbor_core; \\n\\npostgres-# \\\\l *(this will list all the database)*\\n\\n<script src=\\"https://gist.github.com/techslaves/b7bc2a5ea2a3c347324d375b6a49e04b.js\\"><\/script>\\n\\n### Architecture of helm chart deployed \\n\\nImages used while deploying this helm chart:\\n\\n#### *1) bitnami/postgresql-repmgr :*\\nPostgreSQL is an open source object-relational database known for its reliability and data integrity. This solution includes repmgr, an open-source tool for managing replication and failover on PostgreSQL clusters.\\n\\n#### *2) bitnami/pgpool :*\\nPgpool-II is a PostgreSQL proxy. It stands between PostgreSQL servers and their clients providing connection pooling, load balancing, automated failover, and replication.\\n\\nAfter you deploy PostgreSQL HA using helm chart it will deploy two Pod\'s of *pgpool* and  three pods of *postgresql-repmgr*\\npgpool is a deployment and postgresql-repmgr is a stateful set.\\n\\nThis will create one master and two slave cluster where Pgpool will be responsible for promoting slave to master incase master goes down.\\n\\nMaster (pod/postgresql-harbor-postgresql-0) will be single point for write operation where as master and two slaves (pod/postgresql-harbor-postgresql-1 and pod/postgresql-harbor-postgresql-2) will be three point for read operation.\\n\\npod/postgresql-harbor-pgpool-23243g54f8-afdsc \\\\\\npod/postgresql-harbor-pgpool-11834437d4-ojgfd  \\\\\\npod/postgresql-harbor-postgresql-0 \\\\\\npod/postgresql-harbor-postgresql-1  \\\\\\npod/postgresql-harbor-postgresql-2\\n\\nTwo Cluster IP service will be created and one headless service.\\n\\nPGPool pod\'s can be reached through pgpool cluster IP service and postgreSQL pods communicate to postgresql service.\\n\\n#### *For integration with harbor HA pass Pgpool Cluster IP as PG internally communicates to postgreSQL master and slave using headless service.*\\n\\n\\nIn the next post I will be deploying Harbor Helm and details of Integration with Redis and PostgreSQL cluster - \\\\\\n[Kubernetes Dynamic Provisioning - Persistent Volume on demand (Using helm charts)](2021-05-06-Dynamic%20Volume%20Provisioning%20NFS.md)"},{"id":"/2021/05/06/Dynamic Volume Provisioning NFS","metadata":{"permalink":"/blog/2021/05/06/Dynamic Volume Provisioning NFS","editUrl":"https://github.com/techslaves/techslaves.github.io/tree/master/website/blog/2021-05-06-Dynamic Volume Provisioning NFS.md","source":"@site/blog/2021-05-06-Dynamic Volume Provisioning NFS.md","title":"Post 02 - Kubernetes Dynamic Provisioning, Persistent Volume on demand (Using helm charts)","description":"In this post I will be covering details of configuring NFS dynamic provisioner on Bare Metal VM\'s. This article is part of series of Harbor High Availability","date":"2021-05-06T17:40:40.000Z","tags":[{"inline":true,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":2.94,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Post 02 - Kubernetes Dynamic Provisioning, Persistent Volume on demand (Using helm charts)","excerpt":"In this post we will cover how to create storage volume (Persistent Volume) on demand in Kubernetes.","date":"2021-05-06 17:40:40 +0530","tags":["Kubernetes"]},"unlisted":false,"prevItem":{"title":"Post 04 - PostgreSQL High Availability deployment on Kubernetes","permalink":"/blog/2021/05/15/PostgeSQL High Availability deployment on Kubernetes"},"nextItem":{"title":"Bootstrap Kubernetes cluster with PV as NFS","permalink":"/blog/2021/01/14/Kubernetes"}},"content":"In this post I will be covering details of configuring NFS dynamic provisioner on Bare Metal VM\'s. This article is part of series of Harbor High Availability\\n\\n1) Harbor High Availability deployment on Kubernetes \\\\\\n[Post 01 - Harbor High Availability deployment on Kubernetes](2021-05-14-Harbor%20High%20Availability%20deployment%20on%20Kubernetes.md) \\\\\\n\\n2) Kubernetes Dynamic Provisioning - Persistent Volume on demand (Using helm charts) (this post)\\n\\n3) High available Redis (Using helm charts)\\n\\n4) High available PostgreSQL database (Using helm charts) \\\\\\n[Post 04 - PostgreSQL High Availability deployment on Kubernetes](2021-05-15-PostgeSQL%20High%20Availability%20deployment%20on%20Kubernetes.md) \\\\\\n\\n5) Harbor High Availability deployment in action on Kubernetes using helm chart. \\\\\\n\\n#### *This article can also be used to configure NFS Dynamic Provisioning even if you are not installing Harbor High Availability.*\\n\\n**Prerequisite**\\n\\nNFS server is up and running.\\n\\nRead my previous post to create NFS server and bootstrap kubernetes using kubeadm.\\n\\n- [Bootstrap Kubernetes cluster with PV as NFS ](2021-01-14-Kubernetes.md)\\n\\n***What is NFS dynamic provisioning***\\n\\nNFS dynamic provisioning allows PersistentVolume to be created on demand. There can be multiple storage class within a Kubernetes cluster which provide dynamic provisioning.\\n\\nOne option is we have default storage class configured for the cluster and each time a PVC is raised a PV is automatically created.\\nStorageclass can be marked as default using annotations under metadata.\\n\\nannotations:\\n    storageclass.kubernetes.io/is-default-class=true\\n\\nOther option is we have multiple dynamic provisioner configured and each time a PVC is raised it should be mentioned in yaml which provisioner user want to use to create underlying PV.\\nThis can be achieved through mentioning storageclass in the yaml.\\n\\nUsually each cloud provider provides a Volume plugin which is used as provisioner in storageclass yaml to define where underlying PV will be created for PVC on demand.\\n\\n***Using helm chart for dynamic provisioning***\\n\\n<script src=\\"https://gist.github.com/techslaves/7f75665d4e31f68bd6ab9895dc47382f.js\\"><\/script>\\n\\t\\n**Default values of helm chart which are modified:**\\n\\n*storageClass.reclaimPolicy :*\\nDefault value is \\"Delete\\", incase if you want to retain the pv for future use override this value to \\"Retain\\". This will help to reclaim the obsoleted volume\\nFor instance you have a statefulset and you reduce the number of replica.\\nThen again increase the number of replicas the old pv will still be lying and will automatically be attached with the new increased replica.\\n\\t\\t\\t\\t\\t\\t\\t \\n*storageClass.archiveOnDelete :*\\ndefault value is true, which will result in archiving the pv data lying on NFS server.\\n\\nAbove two overriden values will help to retain data with pv incase you uninstall and reinstall a same helm chart.\\n\\n*replicaCount :*\\nIncreasing replica count to 3 for high availability. Which defaults to 1.\\n\\n*storageClass.accessModes :*\\ndefault value is ReadWriteOnce(RWO). Other posssible values are ReadOnlyMany(ROX) and ReadWriteMany(RWX)\\nReadWriteMany should be used when you want multiple pods(lying on multiple nodes) to write on same pv.\\nNFS, CephFS and Glusterfs only supports all three types of access modes.\\n\\nRun $kubectl get sc\\n\\nYou should be able to see storage class with name `nfs-client`.\\n\\n*Using --kubeconfig parameter as I am installing helm chart from powershell from my Windows laptop on remote kubernetes cluster.*\\n\\n**Test automatic PV creation on NFS server***\\n\\nCreate below PVC and check if underlying PV is created on NFS server automatically\\n\\n<script src=\\"https://gist.github.com/techslaves/5212e35f209a71a347d3c0b7bd576103.js\\"><\/script>\\n\\nRun $  kubectl get pv, pvc\\n\\nAbove command will show PVC created from above yaml and underlying pv.\\nGo to the nfs server on mount directory and one can find underlying pv folder.\\n\\n**NOTE**\\n This usecase was implemented was done on bare-metal cluster."},{"id":"/2021/01/14/Kubernetes","metadata":{"permalink":"/blog/2021/01/14/Kubernetes","editUrl":"https://github.com/techslaves/techslaves.github.io/tree/master/website/blog/2021-01-14-Kubernetes.md","source":"@site/blog/2021-01-14-Kubernetes.md","title":"Bootstrap Kubernetes cluster with PV as NFS","description":"Create three ec2 instance","date":"2021-01-14T17:40:40.000Z","tags":[{"inline":true,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":1.73,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Bootstrap Kubernetes cluster with PV as NFS","excerpt":"In this post we will cover how to Bootstrap Kubernetes cluster using Kubeadm with underlying persistent volume as NFS","date":"2021-01-14 17:40:40 +0530","tags":["Kubernetes"]},"unlisted":false,"prevItem":{"title":"Post 02 - Kubernetes Dynamic Provisioning, Persistent Volume on demand (Using helm charts)","permalink":"/blog/2021/05/06/Dynamic Volume Provisioning NFS"}},"content":"**Create three ec2 instance**\\n\\nMaster node : centos \\\\\\nWorker node: centos \\\\\\nNFS server : ubuntu\\n\\n**Steps to create NFS Server on ubuntu instance**\\n\\n<script src=\\"https://gist.github.com/techslaves/70de4a76c0d72bf006c7d0572a081d1f.js\\"><\/script>\\n\\n![](/img/kubernetes/post01/NFS1.JPG)\\n\\n![](/img/kubernetes/post01/NFS2.JPG)\\n\\nMake sure you have below inbound rules for NFS client to ping to NFS server and mount directory on client-server\\n\\n![](/img/kubernetes/post01/NFS3.JPG)\\n\\n**Setup Master node (control plane) - Install Docker and Kubernetes (kubeadm, kubectl, kubelet)**\\n\\n<script src=\\"https://gist.github.com/techslaves/b5830e4a9155e3f85fb199ebb5774329.js\\"><\/script>\\n\\n![](/img/kubernetes/post01/NFS4.JPG)\\n\\n![](/img/kubernetes/post01/NFS5.JPG)\\n\\nAdd below Inbound rules with masternode of kubernetes cluster\\n\\n![](/img/kubernetes/post01/NFS6.JPG)\\n\\n**Setup Worker Node**\\n\\nInstall Docker, kubeadm, kubelet and kubectl as done in above steps for master node\\n\\n1) Login to worker node\\n\\n2) Add below inbound rules on worker nodes (Without adding port TCP/UDP 2049/111 you won\'t be able to mount worker node on NFSdirectory)\\n\\n![](/img/kubernetes/post01/NFS7.JPG)\\n\\n3) Run token which you got from kubeadm init command:\\n `$ kubeadm join 1X2.3X.4.XXX:6443 --token XXXXXXXXXXXXXXXXXXXXXXX\\\\ --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXX`\\n\\n4) `$ sudo yum install nfs-util `\\n\\n![](/img/kubernetes/post01/NFS8.JPG)\\n\\n![](/img/kubernetes/post01/NFS9.JPG)\\n\\n5) `mount -t nfs :/srv/nfs/mydata /mnt `\\n6) `mount | grep mydata`\\n\\n![](/img/kubernetes/post01/NFS10.JPG)\\n\\n***Create PV, PVC and POD\'s to Use NFS server as underlying storage:***\\n\\n1) Login back to master node \\n\\n2) Run\\n$kubectl get nodes (this should list one master and worker node as Ready) \\n\\n*Create default nfs-storageclass.yaml* \\n<script src=\\"https://gist.github.com/techslaves/44c423d5d887c1941df3ab23cbd37d3f.js\\"><\/script>\\n\\n\\n***Create nfs-pv.yaml file with below contents***\\n<script src=\\"https://gist.github.com/techslaves/3bf0f8f6728cd46db7c9dead33bf5168.js\\"><\/script>\\n\\n\\n***Create pvc.yaml***\\n<script src=\\"https://gist.github.com/techslaves/30229b3dfc3777d66cfdb3543215c06c.js\\"><\/script>\\n\\n***Create nfs-deployment.yaml***\\n<script src=\\"https://gist.github.com/techslaves/d1999bb64a02e767fb1f16c673e4af33.js\\"><\/script>\\n\\n<script src=\\"https://gist.github.com/techslaves/0a86d2c1479ce24bfa9158bf2499afef.js\\"><\/script>\\n\\nAny file created on location /usr/share/nginx/html will be backed up on NFS server location /srv/nfs/mydata\\n\\n***NOTE***\\n** Contents of yaml can be found at [TechSlaves Repo: deployment-pv-nfs](https://github.com/techslaves/kubernetes/tree/main/deployment-pv-nfs) **"}]}}')}}]);